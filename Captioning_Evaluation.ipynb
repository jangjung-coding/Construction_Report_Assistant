{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "11EBcTaI_7uhbOqKilkTHS2_59uSdjEv7",
      "authorship_tag": "ABX9TyPQSPW50t/CcaqTudtE8tQP"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "FpGtuKKnCFWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sheet_name = 'captions_records_1000' # replace with your own sheet name\n",
        "sheet_id = '1HOOTWxHksen9sOZTBCbRj66f10ULlgrnw1V8jVWurp8' # replace with your sheet's ID"
      ],
      "metadata": {
        "id": "PNc4MZ-UCyaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = f\"https://docs.google.com/spreadsheets/d/{sheet_id}/gviz/tq?tqx=out:csv&sheet={sheet_name}\""
      ],
      "metadata": {
        "id": "xIUEnzcYCHIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(url)\n",
        "print(data.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UxKhX69ODC5S",
        "outputId": "550a4d64-47d7-43e6-dd2a-dfa87ff6e38f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  image_id  reference_caption  generated_caption\n",
            "0  image_1                NaN                NaN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BLEU(Bilingual Evaluation Understudy)"
      ],
      "metadata": {
        "id": "t7x1_ZXu2vEo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "BLEU(Bilingual Evaluation Understudy) 점수는 기계 번역이나 자연어 생성 작업의 정량적 평가 방법 중 하나로, 생성된 텍스트가 참조 텍스트(reference text)와 얼마나 유사한지 측정하는 지표입니다. 주로 단어나 구문이 얼마나 일치하는지를 평가합니다."
      ],
      "metadata": {
        "id": "UJfm0pmWyWEW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5kyUEKvaxpWa",
        "outputId": "ab59f888-2eb7-4514-9e5a-483fba1d0b5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Smoothing function to avoid zero BLEU scores for short sentences\n",
        "smoothie = SmoothingFunction().method4\n",
        "\n",
        "# CSV 파일 불러오기\n",
        "df = pd.read_csv('/content/drive/MyDrive/잡동사니/captions_records_1000.csv')\n",
        "\n",
        "# BLEU 점수를 저장할 리스트\n",
        "bleu_scores = []\n",
        "\n",
        "# 전체 이미지 리스트\n",
        "unique_images = df['image_id'].unique()\n",
        "\n",
        "for image_id in unique_images:\n",
        "    # 해당 이미지의 참조 캡셔닝과 생성된 캡셔닝을 가져오기\n",
        "    image_data = df[df['image_id'] == image_id]\n",
        "    reference_captions = image_data['reference_caption'].apply(lambda x: x.split()).tolist()  # 참조 캡셔닝을 단어 리스트로 변환\n",
        "    generated_captions = image_data['generated_caption'].apply(lambda x: x.split()).tolist()  # 생성된 캡셔닝을 단어 리스트로 변환\n",
        "\n",
        "    # 이미지에 대한 BLEU 점수 계산\n",
        "    for generated_caption in generated_captions:\n",
        "        score = sentence_bleu(reference_captions, generated_caption, smoothing_function=smoothie)\n",
        "        bleu_scores.append(score)\n",
        "\n",
        "# 전체 BLEU 점수 평균 계산\n",
        "average_bleu = sum(bleu_scores) / len(bleu_scores)\n",
        "print(f\"전체 BLEU 점수 평균: {average_bleu * 100:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7Coe0An1sLs",
        "outputId": "8719cd43-dfd2-4372-8ad0-29f0f2640431"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "전체 BLEU 점수 평균: 13.41\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BLEU 점수 해석\n",
        "\n",
        "1.0 (100%): 완벽히 동일한 문장이 생성된 경우입니다. 생성된 텍스트가 참조 텍스트와 완전히 일치할 때 얻을 수 있는 최고 점수입니다.\n",
        "\n",
        "\n",
        "0.0 (0%): 생성된 문장이 참조 문장과 전혀 일치하지 않는 경우입니다.\n",
        "\n",
        "일반적인 점수 범위: BLEU 점수는 보통 0.0에서 1.0 사이의 값을 가지며(0%~100%), 30% 이상의 점수는 일반적으로 생성된 텍스트가 참조 텍스트와 꽤 유사하다는 의미입니다.\n",
        "\n",
        "---\n",
        "- 0.0 ~ 10.0: 매우 낮은 성능. 생성된 문장이 참조 문장과 거의 유사하지 않음.\n",
        "- 10.0 ~ 30.0: 낮은 성능. 생성된 문장이 참조 문장과 다소 차이가 있음. 초보적인 모델에서 자주 볼 수 있음.\n",
        "- 30.0 ~ 50.0: 중간 성능. 생성된 문장이 참조 문장과 어느 정도 유사하지만, 여전히 차이가 있음.\n",
        "- 50.0 ~ 70.0: 좋은 성능. 생성된 문장이 참조 문장과 상당히 유사하며, 대부분의 중요한 정보가 일치함.\n",
        "- 70.0 ~ 85.0: 매우 좋은 성능. 생성된 문장이 거의 참조 문장과 일치하며, 높은 수준의 성능을 나타냄.\n",
        "- 85.0 이상: 완벽에 가까운 성능. 생성된 문장이 참조 문장과 거의 동일함."
      ],
      "metadata": {
        "id": "GMedO4DBy1m8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ROUGE (Recall-Oriented Understudy for Gisting Evaluation)"
      ],
      "metadata": {
        "id": "7m1Gk3fp2yEV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ROUGE (Recall-Oriented Understudy for Gisting Evaluation)는 자동 요약 및 자연어 처리에서 생성된 텍스트를 평가하기 위해 BLEU와 함께 자주 사용되는 평가 지표입니다."
      ],
      "metadata": {
        "id": "IkO7ofQ220e-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ROUGE-1: 단일 단어(1-gram)의 일치율을 측정합니다.\n",
        "\n",
        "ROUGE-2: 2-gram(단어의 연속된 두 단어)의 일치율을 측정합니다.\n",
        "\n",
        "ROUGE-L: LCS(가장 긴 공통 서브시퀀스)를 기반으로 비교합니다."
      ],
      "metadata": {
        "id": "Jlj5e2TI3J8m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "평균 점수 계산: 각 캡셔닝에 대해 계산된 ROUGE 점수들을 모두 더한 후, 그 평균을 계산하여 최종 ROUGE-1, ROUGE-2, ROUGE-L 평균 점수를 출력합니다."
      ],
      "metadata": {
        "id": "v6A3d-vW3Ott"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge-score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MY6-_gyV20Bb",
        "outputId": "5d209c60-2b55-45f3-9e2e-0438dd140005"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge-score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge-score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.26.4)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (4.66.6)\n",
            "Building wheels for collected packages: rouge-score\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=0ecc26dae23a1aa3188099f060b9d24c9af5635844eaa92e136cb5cebb0fcae4\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "Successfully built rouge-score\n",
            "Installing collected packages: rouge-score\n",
            "Successfully installed rouge-score-0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from rouge_score import rouge_scorer\n",
        "\n",
        "# ROUGE 평가를 위한 Scorer 초기화 (ROUGE-1, ROUGE-2, ROUGE-L을 계산)\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "# ROUGE 점수를 저장할 리스트\n",
        "rouge_scores = {\n",
        "    'rouge1': [],\n",
        "    'rouge2': [],\n",
        "    'rougeL': []\n",
        "}\n",
        "\n",
        "# 전체 이미지 리스트\n",
        "unique_images = df['image_id'].unique()\n",
        "\n",
        "for image_id in unique_images:\n",
        "    # 해당 이미지의 참조 캡셔닝과 생성된 캡셔닝을 가져오기\n",
        "    image_data = df[df['image_id'] == image_id]\n",
        "    reference_captions = image_data['reference_caption']\n",
        "    generated_captions = image_data['generated_caption']\n",
        "\n",
        "    # 각 생성된 캡셔닝에 대해 ROUGE 점수 계산\n",
        "    for ref_cap, gen_cap in zip(reference_captions, generated_captions):\n",
        "        scores = scorer.score(ref_cap, gen_cap)\n",
        "        rouge_scores['rouge1'].append(scores['rouge1'].fmeasure)\n",
        "        rouge_scores['rouge2'].append(scores['rouge2'].fmeasure)\n",
        "        rouge_scores['rougeL'].append(scores['rougeL'].fmeasure)\n",
        "\n",
        "# 각 ROUGE 스코어 평균 계산\n",
        "average_rouge1 = sum(rouge_scores['rouge1']) / len(rouge_scores['rouge1'])\n",
        "average_rouge2 = sum(rouge_scores['rouge2']) / len(rouge_scores['rouge2'])\n",
        "average_rougeL = sum(rouge_scores['rougeL']) / len(rouge_scores['rougeL'])\n",
        "\n",
        "print(f\"ROUGE-1 평균 점수: {average_rouge1 * 100:.2f}\")\n",
        "print(f\"ROUGE-2 평균 점수: {average_rouge2 * 100:.2f}\")\n",
        "print(f\"ROUGE-L 평균 점수: {average_rougeL * 100:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISyKAr4928RK",
        "outputId": "e5c65c70-e82a-4e7d-a500-28ccd28e2e2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROUGE-1 평균 점수: 100.00\n",
            "ROUGE-2 평균 점수: 0.00\n",
            "ROUGE-L 평균 점수: 100.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ROUGE 평가 결과 해석\n",
        "\n",
        "ROUGE-1: 단어 일치율을 평가하는 지표로, 점수가 높을수록 생성된 캡셔닝이 참조 캡셔닝과 단어 수준에서 유사하다는 것을 의미합니다.\n",
        "\n",
        "ROUGE-2: 2-gram 일치율을 평가하는 지표로, 점수가 높을수록 두 단어의 연속된 묶음이 참조 캡셔닝과 유사하게 생성되었음을 의미합니다.\n",
        "\n",
        "ROUGE-L: 가장 긴 공통 서브시퀀스를 평가하는 지표로, 점수가 높을수록 전체 문장에서 긴 구문 구조가 참조 캡셔닝과 유사하다는 것을 의미합니다.\n",
        "\n",
        "---\n",
        "- 0.0 ~ 10.0: 매우 낮은 성능. 생성된 텍스트가 참조 텍스트와 거의 일치하지 않음.\n",
        "\n",
        "- 10.0 ~ 30.0: 낮은 성능. 생성된 텍스트가 참조 텍스트와 다소 차이가 있음.\n",
        "\n",
        "- 30.0 ~ 50.0: 중간 성능. 생성된 텍스트가 참조 텍스트와 어느 정도 유사하지만, 여전히 차이가 있음.\n",
        "\n",
        "- 50.0 ~ 70.0: 좋은 성능. 생성된 텍스트가 참조 텍스트와 상당히 유사하며, 중요한 정보가 일치함.\n",
        "\n",
        "- 70.0 ~ 85.0: 매우 좋은 성능. 생성된 텍스트가 참조 텍스트와 거의 일치함.\n",
        "\n",
        "- 85.0 이상: 거의 완벽한 성능."
      ],
      "metadata": {
        "id": "hyU6XYVz3RbO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# METEOR(Metric for Evaluation of Translation with Explicit ORdering)"
      ],
      "metadata": {
        "id": "XvyReLug36B9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "METEOR(Metric for Evaluation of Translation with Explicit ORdering)는 BLEU와 ROUGE와 함께 자주 사용되는 자연어 처리(NLP) 평가 지표 중 하나입니다. METEOR는 단어의 어근, 동의어, 그리고 단어 순서까지 고려하여 평가를 진행하므로, BLEU나 ROUGE보다 더 의미 중심적인 평가를 할 수 있습니다."
      ],
      "metadata": {
        "id": "USjgt0bB39h9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akpGwc6X39FX",
        "outputId": "19a8f8c2-b462-496b-8a01-62b570f0ee14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "\n",
        "# NLTK에서 WordNet 리소스를 다운로드\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# CSV 파일 불러오기\n",
        "df = pd.read_csv('/content/drive/MyDrive/잡동사니/captions_records_1000.csv')\n",
        "\n",
        "# METEOR 점수를 저장할 리스트\n",
        "meteor_scores = []\n",
        "\n",
        "# 전체 이미지 리스트\n",
        "unique_images = df['image_id'].unique()\n",
        "\n",
        "for image_id in unique_images:\n",
        "    # 해당 이미지의 참조 캡셔닝과 생성된 캡셔닝을 가져오기\n",
        "    image_data = df[df['image_id'] == image_id]\n",
        "    reference_captions = image_data['reference_caption'].tolist()  # 참조 캡셔닝 리스트\n",
        "    generated_captions = image_data['generated_caption'].tolist()  # 생성된 캡셔닝 리스트\n",
        "\n",
        "    # 각 생성된 캡셔닝에 대해 METEOR 점수 계산\n",
        "    for ref_cap, gen_cap in zip(reference_captions, generated_captions):\n",
        "        # 참조 캡셔닝과 생성된 캡셔닝 모두를 단어 단위로 토큰화\n",
        "        ref_cap_tokens = ref_cap.split()  # 참조 캡셔닝을 단어로 분리\n",
        "        gen_cap_tokens = gen_cap.split()  # 생성된 캡셔닝을 단어로 분리\n",
        "\n",
        "        # METEOR 점수를 계산하여 리스트에 추가\n",
        "        score = meteor_score([ref_cap_tokens], gen_cap_tokens)  # 토큰화된 리스트로 전달\n",
        "        meteor_scores.append(score)\n",
        "\n",
        "# 전체 METEOR 점수 평균 계산\n",
        "average_meteor = sum(meteor_scores) / len(meteor_scores)\n",
        "print(f\"전체 METEOR 점수 평균: {average_meteor * 100:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3E1CUMG04FwP",
        "outputId": "e7a330c9-e2f5-4a39-95e5-745a173f07c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "전체 METEOR 점수 평균: 51.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 0.0 ~ 10.0: 매우 낮은 성능. 생성된 문장이 참조 문장과 거의 유사하지 않음.\n",
        "- 10.0 ~ 30.0: 낮은 성능. 생성된 문장이 참조 문장과 다소 차이가 있음.\n",
        "- 30.0 ~ 50.0: 중간 성능. 생성된 문장이 참조 문장과 어느 정도 유사하지만, 여전히 차이가 있음.\n",
        "- 50.0 ~ 70.0: 좋은 성능. 생성된 문장이 참조 문장과 상당히 유사하며, 중요한 정보가 일치함.\n",
        "- 70.0 이상: 매우 좋은 성능. 생성된 문장이 참조 문장과 거의 일치하며, 높은 수준의 의미적 유사성을 나타냄."
      ],
      "metadata": {
        "id": "Nw-xClFU4CP1"
      }
    }
  ]
}